{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "[] interpret coeff & feature importance\n",
    "\n",
    "[] model without pca (weak correlations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Modeling\n",
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score, roc_curve, roc_auc_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "\n",
    "Introduction\n",
    "\n",
    "\n",
    "In this notebook, I will be building two types classification models. Using Logistic Regression and Decision Trees, we will see if we can correctly classify our target variable and predict the classes of new datapoints.\n",
    "I will train each model with a portion of the dataset and then compare the outputs with my test set. \n",
    "\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart22 = pd.read_csv('~/Desktop/capstone-project-Tasnimacj/data/cleaned_data/heart22_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 246013 entries, 0 to 246012\n",
      "Data columns (total 43 columns):\n",
      " #   Column                         Non-Null Count   Dtype  \n",
      "---  ------                         --------------   -----  \n",
      " 0   Unnamed: 0                     246013 non-null  int64  \n",
      " 1   Female                         246013 non-null  int64  \n",
      " 2   GeneralHealth                  246013 non-null  int64  \n",
      " 3   PhysicalHealthDays             246013 non-null  float64\n",
      " 4   MentalHealthDays               246013 non-null  float64\n",
      " 5   LastCheckupTime                246013 non-null  int64  \n",
      " 6   PhysicalActivities             246013 non-null  int64  \n",
      " 7   SleepHours                     246013 non-null  float64\n",
      " 8   RemovedTeeth                   246013 non-null  int64  \n",
      " 9   HadHeartAttack                 246013 non-null  int64  \n",
      " 10  HadAngina                      246013 non-null  int64  \n",
      " 11  HadStroke                      246013 non-null  int64  \n",
      " 12  HadAsthma                      246013 non-null  int64  \n",
      " 13  HadSkinCancer                  246013 non-null  int64  \n",
      " 14  HadCOPD                        246013 non-null  int64  \n",
      " 15  HadDepressiveDisorder          246013 non-null  int64  \n",
      " 16  HadKidneyDisease               246013 non-null  int64  \n",
      " 17  HadArthritis                   246013 non-null  int64  \n",
      " 18  HadDiabetes                    246013 non-null  int64  \n",
      " 19  DeafOrHardOfHearing            246013 non-null  int64  \n",
      " 20  BlindOrVisionDifficulty        246013 non-null  int64  \n",
      " 21  DifficultyConcentrating        246013 non-null  int64  \n",
      " 22  DifficultyWalking              246013 non-null  int64  \n",
      " 23  DifficultyDressingBathing      246013 non-null  int64  \n",
      " 24  DifficultyErrands              246013 non-null  int64  \n",
      " 25  SmokerStatus                   246013 non-null  int64  \n",
      " 26  ECigaretteUsage                246013 non-null  int64  \n",
      " 27  ChestScan                      246013 non-null  int64  \n",
      " 28  AgeCategory                    246013 non-null  int64  \n",
      " 29  HeightInMeters                 246013 non-null  float64\n",
      " 30  WeightInKilograms              246013 non-null  float64\n",
      " 31  BMI                            246013 non-null  float64\n",
      " 32  AlcoholDrinkers                246013 non-null  int64  \n",
      " 33  HIVTesting                     246013 non-null  int64  \n",
      " 34  FluVaxLast12                   246013 non-null  int64  \n",
      " 35  PneumoVaxEver                  246013 non-null  int64  \n",
      " 36  TetanusLast10Tdap              246013 non-null  int64  \n",
      " 37  HighRiskLastYear               246013 non-null  int64  \n",
      " 38  CovidPos                       246013 non-null  int64  \n",
      " 39  RaceEthnicity_Black only       246013 non-null  int64  \n",
      " 40  RaceEthnicity_Hispanic         246013 non-null  int64  \n",
      " 41  RaceEthnicity_Multiracial      246013 non-null  int64  \n",
      " 42  RaceEthnicity_Other race only  246013 non-null  int64  \n",
      "dtypes: float64(6), int64(37)\n",
      "memory usage: 80.7 MB\n"
     ]
    }
   ],
   "source": [
    "heart22.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I load in the data that we saved from the previous notebook. Checking if our dataset is properly encoded, with 0 null values and has a correct index. Everything looks good, so we can move straight into modeling now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = heart22['HadAngina'] # Target Variable\n",
    "X = heart22.drop('HadAngina', axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y: (246013,)\n",
      "Shape of X: (246013, 42)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of y:', y.shape)\n",
    "print('Shape of X:', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split our data into our feature columns and our target variable. We will be putting our X into our models and comparing the outputs with our y.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The remainder set has 196810 data points.\n",
      "The test set has 49203 data points.\n"
     ]
    }
   ],
   "source": [
    "#1st split\n",
    "\n",
    "X_rem, X_test, y_rem, y_test = train_test_split(X, y, test_size=0.2, random_state=25, stratify=y)\n",
    "\n",
    "print(f'The remainder set has {len(X_rem)} data points.')\n",
    "print(f'The test set has {len(X_test)} data points.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model, we need to split our data into a training set and a test set. Here I did a 80:20 split, keeping an even distribution of y variables in each split. \n",
    "We use the train set to fit the model and then evaluate on the test set. Splitting the datapoints helps prevent overfitting and a way to accurately check model performance. By having unseen data, we can come close to replicating real life scenarios that the model will face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### 1 Baseline Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my first model, I will attempt to create a model that has no hyperparameter optimization. This will be a a model we can use for comparison. I am expecting that this model will perform poorly and will be very good at predicting 0, or 'did not have angina'\n",
    "As logistic regression takes L2 Regularisation as its default penalty, I will need to scale my data beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler().fit(X_rem)\n",
    "X_rem_ss = ss.transform(X_rem)\n",
    "X_test_ss = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=1)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_log_reg = LogisticRegression(random_state=1)\n",
    "baseline_log_reg.fit(X_rem_ss, y_rem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate the model and then fit the model on our y remainder data and the scaled X remainder data. We then input unknown data and compare accuracy scores with our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on remainder set: 0.9450586860423759\n",
      "Accuracy on test set: 0.9446375221023109\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy on remainder set: {baseline_log_reg.score(X_rem_ss, y_rem)}')\n",
    "print(f'Accuracy on test set: {baseline_log_reg.score(X_test_ss, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy on the remainder and test set is the same, 94.5%. This means our model has correctly classified 94.5% of the data points in both sets. This shows that the model has a good fit, and can handle unseen data.\n",
    "\n",
    "However, accuracy is not a good indicator of the model outcomes as it does not show if our model is behaving how we expect. \n",
    "We can look further by using a classification report and confusion matrix to see what datapoints were classified as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted 0</th>\n",
       "      <th>predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true 0</th>\n",
       "      <td>45642</td>\n",
       "      <td>570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true 1</th>\n",
       "      <td>2154</td>\n",
       "      <td>837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        predicted 0  predicted 1\n",
       "true 0        45642          570\n",
       "true 1         2154          837"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall score: 27.98%\n",
      "Precision score: 59.49%\n",
      "F1 score: 38.06%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_test_pred = baseline_log_reg.predict(X_test_ss)\n",
    "\n",
    "\n",
    "cmat = pd.DataFrame(\n",
    "    data = confusion_matrix(y_test, y_test_pred),\n",
    "    index = ['true 0', 'true 1'],\n",
    "    columns = ['predicted 0', 'predicted 1']\n",
    ")\n",
    "display(cmat)\n",
    "\n",
    "\n",
    "print(f'Recall score: {recall_score(y_test, y_test_pred)*100:0.2f}%')\n",
    "print(f'Precision score: {precision_score(y_test, y_test_pred)*100:0.2f}%')\n",
    "print(f'F1 score: {f1_score(y_test, y_test_pred)*100:0.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97     46212\n",
      "           1       0.59      0.28      0.38      2991\n",
      "\n",
      "    accuracy                           0.94     49203\n",
      "   macro avg       0.77      0.63      0.68     49203\n",
      "weighted avg       0.93      0.94      0.94     49203\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though our model was 94.5% accurate, it was only good at predicting 'Had no Angina'. This is most likely due to the imbalance in our dataset. It has seen more cases where y would be '0' than when y would be '1'.\n",
    "\n",
    " My main goal is to get high recall without sacrificing precision and accuracy. For my problem, it would be better to have more false positives than false negatives. It would be worse to mischaracterise someone as not at risk than saying they could be at risk. My model could only classify 837 datapoints correcty as '1' out of 2991 '1' values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### 2 Tuning Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a pipeline, I want to find the best hyperparameters for my model. I set up a pipeline that contains a scaler, dimension reducer and a model of our choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import mkdtemp\n",
    "cachedir = mkdtemp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([(\"scaler\", StandardScaler()),\n",
    "                 (\"my_pca\", PCA(n_components=20)),\n",
    "                 (\"model\", LogisticRegression())], memory=cachedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up a parameter grid with what I would like to change in my model. I want to explore different c values and the model penalty. For now, I only want to scale my data using a standard scaler and look at the top 20 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c_values = [.0001, .001, .01, .1, 1, 10, 100, 1000, 10000]\n",
    "\n",
    "\n",
    "log_reg_param = [\n",
    "\n",
    "    {'scaler': [ StandardScaler()],\n",
    "     'my_pca__n_components': [20],\n",
    "     'model': [LogisticRegression(solver='saga',random_state=1, n_jobs=-1, max_iter=10000)], \n",
    "     'model__C': c_values,\n",
    "     'model__penalty': ['l1', 'l2'],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(estimator=pipe,param_grid=log_reg_param, cv=5,verbose=1,refit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    }
   ],
   "source": [
    "fittedgrid_lr = grid.fit(X_rem,y_rem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': LogisticRegression(C=0.001, max_iter=10000, n_jobs=-1, random_state=1,\n",
       "                    solver='saga'),\n",
       " 'model__C': 0.001,\n",
       " 'model__penalty': 'l2',\n",
       " 'my_pca__n_components': 20,\n",
       " 'scaler': StandardScaler()}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fittedgrid_lr.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearch looks at different parameter settings across 5 cross folds on our Remainder set. It does this to find the best model settings whilst preventing data leakage and overfitting on our train set. \n",
    "GridSearch has found that the best parameters for our Logistic Regression is having a C value of 0.001, which means we need a strong regularization strength. We need a l2 penalty to help shrink coefficients to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy on the remainder set: 0.9424876784716224\n",
      "Best accuracy on the test set: 0.9429709570554641\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best accuracy on the remainder set: {fittedgrid_lr.score(X_rem, y_rem)}\")\n",
    "print(f\"Best accuracy on the test set: {fittedgrid_lr.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted 0</th>\n",
       "      <th>predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true 0</th>\n",
       "      <td>45845</td>\n",
       "      <td>367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true 1</th>\n",
       "      <td>2439</td>\n",
       "      <td>552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        predicted 0  predicted 1\n",
       "true 0        45845          367\n",
       "true 1         2439          552"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall score: 18.46%\n",
      "Precision score: 60.07%\n",
      "F1 score: 28.24%\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = fittedgrid_lr.predict(X_test)\n",
    "\n",
    "conmat = pd.DataFrame(\n",
    "    data = confusion_matrix(y_test, y_test_pred),\n",
    "    index = ['true 0', 'true 1'],\n",
    "    columns = ['predicted 0', 'predicted 1']\n",
    ")\n",
    "display(conmat)\n",
    "\n",
    "print(f'Recall score: {recall_score(y_test, y_test_pred)*100:0.2f}%')\n",
    "print(f'Precision score: {precision_score(y_test, y_test_pred)*100:0.2f}%')\n",
    "print(f'F1 score: {f1_score(y_test, y_test_pred)*100:0.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our decided best logistic regression model is performing worse than our baseline in terms of recall.\n",
    " Perhaps PCA downgraded model performance, we can investigate this further in future notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### 3 Tuning Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the pipeline for another classification model, this time I will use Decision Tree Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([(\"scaler\", StandardScaler()),\n",
    "                 (\"my_pca\", PCA(n_components=20)),\n",
    "                 (\"dt_model\", DecisionTreeClassifier())], memory=cachedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the parameter grid, this time I want to see if scaling data would make a change in modeling. I still 20 components for PCA but explore different options for the trees max_depth and min_samples_leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dt_param  = {\"scaler\":[StandardScaler(), None],\n",
    "            \"my_pca__n_components\":[20],\n",
    "            \"dt_model__max_depth\": [None, 2, 4, 6, 8,10],\n",
    "            \"dt_model__min_samples_leaf\": [2, 5, 10] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'scaler', 'my_pca', 'dt_model', 'scaler__copy', 'scaler__with_mean', 'scaler__with_std', 'my_pca__copy', 'my_pca__iterated_power', 'my_pca__n_components', 'my_pca__random_state', 'my_pca__svd_solver', 'my_pca__tol', 'my_pca__whiten', 'dt_model__ccp_alpha', 'dt_model__class_weight', 'dt_model__criterion', 'dt_model__max_depth', 'dt_model__max_features', 'dt_model__max_leaf_nodes', 'dt_model__min_impurity_decrease', 'dt_model__min_impurity_split', 'dt_model__min_samples_leaf', 'dt_model__min_samples_split', 'dt_model__min_weight_fraction_leaf', 'dt_model__random_state', 'dt_model__splitter'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipe.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(estimator=pipe,param_grid=dt_param, cv=5,verbose=1,refit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    }
   ],
   "source": [
    "fittedgrid_dt = grid.fit(X_rem,y_rem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dt_model__max_depth': 6,\n",
       " 'dt_model__min_samples_leaf': 10,\n",
       " 'my_pca__n_components': 20,\n",
       " 'scaler': StandardScaler()}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fittedgrid_dt.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best optimization for our decision tree would be using a standard scaler, having a max_depth of 6 and 10 min_samples_leaf. Scaling my data beforehand seems to be a crucial step for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy on the remainder set: 0.9402977490981149\n",
      "Best accuracy on the test set: 0.9395768550698128\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best accuracy on the remainder set: {fittedgrid_dt.score(X_rem, y_rem)}\")\n",
    "print(f\"Best accuracy on the test set: {fittedgrid_dt.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracies are, again, similar to each other. We have no issue with over/underfitting. The accuracies being in the 90s shows that it is very confident in predicting classes correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted 0</th>\n",
       "      <th>predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true 0</th>\n",
       "      <td>45967</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true 1</th>\n",
       "      <td>2728</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        predicted 0  predicted 1\n",
       "true 0        45967          245\n",
       "true 1         2728          263"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall score: 8.79%\n",
      "Precision score: 51.77%\n",
      "F1 score: 15.03%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_test_pred = fittedgrid_dt.predict(X_test)\n",
    "\n",
    "conmat = pd.DataFrame(\n",
    "    data = confusion_matrix(y_test, y_test_pred),\n",
    "    index = ['true 0', 'true 1'],\n",
    "    columns = ['predicted 0', 'predicted 1']\n",
    ")\n",
    "display(conmat)\n",
    "\n",
    "print(f'Recall score: {recall_score(y_test, y_test_pred)*100:0.2f}%')\n",
    "print(f'Precision score: {precision_score(y_test, y_test_pred)*100:0.2f}%')\n",
    "print(f'F1 score: {f1_score(y_test, y_test_pred)*100:0.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our recall score has plummeted to 8%. This model is very bad classifying '1's, so it is unusable.\n",
    "The precision is around 50%, so only about half of its '1' predictions were correct.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### 4 SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in EDA, the actual proportion of 'HadAngina' in our target column was very small. To try and combat this we can sample our data, either using Upsampling, Downsampling or SMOTE. Here, I will be using SMOTE to create 'fake' data points by interpolating between datapoints. I hope to see model performances increase as they can learn better from having a more balanced dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rem_sm, y_rem_sm = SMOTE(random_state=1).fit_resample(X_rem, y_rem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution\n",
      "HadAngina\n",
      "0    184848\n",
      "1     11962\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Resampled class distribution\n",
      "HadAngina\n",
      "0    184848\n",
      "1    184848\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Original class distribution')\n",
    "print((y_rem).value_counts().sort_index(),'\\n')\n",
    "\n",
    "print('Resampled class distribution')\n",
    "print((y_rem_sm).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE has inflated our '1' distribution to now be equal with the number of '0's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 LogReg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating the previous steps again but this time fitting with our new balanced remainder set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([(\"scaler\", StandardScaler()),\n",
    "                 (\"my_pca\", PCA(n_components=20)),\n",
    "                 (\"model\", LogisticRegression())], memory=cachedir)\n",
    "log_reg_param = [\n",
    "\n",
    "    {'scaler': [ StandardScaler()],\n",
    "     'my_pca__n_components': [20],\n",
    "     'model': [LogisticRegression(solver='saga',random_state=1, n_jobs=-1, max_iter=10000)], \n",
    "     'model__C': c_values,\n",
    "     'model__penalty': ['l1', 'l2'],\n",
    "    }\n",
    "]\n",
    "\n",
    "grid_sm = GridSearchCV(estimator=pipe,param_grid=log_reg_param, cv=5,verbose=1,refit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    }
   ],
   "source": [
    "fittedgrid_lr_sm = grid_sm.fit(X_rem_sm,y_rem_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy on the remainder set: 0.8598605340604172\n",
      "Best accuracy on the test set: 0.8297868016177875\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted 0</th>\n",
       "      <th>predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true 0</th>\n",
       "      <td>39380</td>\n",
       "      <td>6832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true 1</th>\n",
       "      <td>1543</td>\n",
       "      <td>1448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        predicted 0  predicted 1\n",
       "true 0        39380         6832\n",
       "true 1         1543         1448"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall score: 48.41%\n",
      "Precision score: 17.49%\n",
      "F1 score: 25.69%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Best accuracy on the remainder set: {fittedgrid_lr_sm.score(X_rem_sm, y_rem_sm)}\")\n",
    "print(f\"Best accuracy on the test set: {fittedgrid_lr_sm.score(X_test, y_test)}\")\n",
    "\n",
    "\n",
    "y_test_pred = fittedgrid_lr_sm.predict(X_test)\n",
    "\n",
    "conmat = pd.DataFrame(\n",
    "    data = confusion_matrix(y_test, y_test_pred),\n",
    "    index = ['true 0', 'true 1'],\n",
    "    columns = ['predicted 0', 'predicted 1']\n",
    ")\n",
    "display(conmat)\n",
    "\n",
    "print(f'Recall score: {recall_score(y_test, y_test_pred)*100:0.2f}%')\n",
    "print(f'Precision score: {precision_score(y_test, y_test_pred)*100:0.2f}%')\n",
    "print(f'F1 score: {f1_score(y_test, y_test_pred)*100:0.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the accuracy has dropped, I can see an increase in our recall. The model became more familiar with cases that are classed as '1'  and can recognise it. However, our precision has fallen quite a bit, which shows that the model is not  predicting correctly. I can further investigate and make more changes to find a model with a better recall score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a decision tree with our balanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([(\"scaler\", StandardScaler()),\n",
    "                 (\"my_pca\", PCA(n_components=20)),\n",
    "                 (\"dt_model\", DecisionTreeClassifier())], memory=cachedir)\n",
    "\n",
    "dt_param  = {\"scaler\":[StandardScaler(), None],\n",
    "            \"my_pca__n_components\":[20],\n",
    "            \"dt_model__max_depth\": [None, 2, 4, 6, 8,10],\n",
    "            \"dt_model__min_samples_leaf\": [2, 5, 10] }\n",
    "\n",
    "\n",
    "grid_sm = GridSearchCV(estimator=pipe,param_grid=dt_param, cv=5,verbose=1,refit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    }
   ],
   "source": [
    "fittedgrid_dt_sm = grid_sm.fit(X_rem_sm,y_rem_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy on the remainder set: 0.863314723448455\n",
      "Best accuracy on the test set: 0.8128975875454749\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted 0</th>\n",
       "      <th>predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true 0</th>\n",
       "      <td>38756</td>\n",
       "      <td>7456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true 1</th>\n",
       "      <td>1750</td>\n",
       "      <td>1241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        predicted 0  predicted 1\n",
       "true 0        38756         7456\n",
       "true 1         1750         1241"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall score: 41.49%\n",
      "Precision score: 14.27%\n",
      "F1 score: 21.24%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best accuracy on the remainder set: {fittedgrid_dt_sm.score(X_rem_sm, y_rem_sm)}\")\n",
    "print(f\"Best accuracy on the test set: {fittedgrid_dt_sm.score(X_test, y_test)}\")\n",
    "\n",
    "\n",
    "y_test_pred = fittedgrid_dt_sm.predict(X_test)\n",
    "\n",
    "\n",
    "conmat = pd.DataFrame(\n",
    "    data = confusion_matrix(y_test, y_test_pred),\n",
    "    index = ['true 0', 'true 1'],\n",
    "    columns = ['predicted 0', 'predicted 1']\n",
    ")\n",
    "display(conmat)\n",
    "\n",
    "print(f'Recall score: {recall_score(y_test, y_test_pred)*100:0.2f}%')\n",
    "print(f'Precision score: {precision_score(y_test, y_test_pred)*100:0.2f}%')\n",
    "print(f'F1 score: {f1_score(y_test, y_test_pred)*100:0.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a huge increase in our recall score. So we can see that SMOTE does help with prediction.\n",
    "\n",
    " But our precision score has dropped again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "\n",
    "Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'F1 score' :[38.06,28.24,15.03,25.69,21.24],\n",
    "     'Recall score':[27.98,18.46,8.79,48.41,41.49], \n",
    "     'Precision score':[59.49,60.07,51.77,17.49,14.27],\n",
    "      'Accuracy':[94.46,94.30,93.96,82.98,81.29]}\n",
    "\n",
    "\n",
    "scores = pd.DataFrame(\n",
    "    data = data,\n",
    "    index = ['Baseline LogReg', 'Best LogReg', 'Best DT','Best SMOTE LogReg', 'Best SMOTE DT' ],\n",
    "    columns = ['F1 score','Recall score', 'Precision score', 'Accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a table that compares the F1, Recall, Precision and Test Accuracy for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 score</th>\n",
       "      <th>Recall score</th>\n",
       "      <th>Precision score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline LogReg</th>\n",
       "      <td>38.06</td>\n",
       "      <td>27.98</td>\n",
       "      <td>59.49</td>\n",
       "      <td>94.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Best LogReg</th>\n",
       "      <td>28.24</td>\n",
       "      <td>18.46</td>\n",
       "      <td>60.07</td>\n",
       "      <td>94.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Best DT</th>\n",
       "      <td>15.03</td>\n",
       "      <td>8.79</td>\n",
       "      <td>51.77</td>\n",
       "      <td>93.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Best SMOTE LogReg</th>\n",
       "      <td>25.69</td>\n",
       "      <td>48.41</td>\n",
       "      <td>17.49</td>\n",
       "      <td>82.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Best SMOTE DT</th>\n",
       "      <td>21.24</td>\n",
       "      <td>41.49</td>\n",
       "      <td>14.27</td>\n",
       "      <td>81.29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   F1 score  Recall score  Precision score  Accuracy\n",
       "Baseline LogReg       38.06         27.98            59.49     94.46\n",
       "Best LogReg           28.24         18.46            60.07     94.30\n",
       "Best DT               15.03          8.79            51.77     93.96\n",
       "Best SMOTE LogReg     25.69         48.41            17.49     82.98\n",
       "Best SMOTE DT         21.24         41.49            14.27     81.29"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before sampling, my best model was my baseline model. After sampling, I found that my recall improves, which is what I ultimately want.  In future notebooks, I will be looking at modelling without PCA, and trying different hyperparameters. \n",
    "I will have to look into different combinations for my model that would give me a good recall score but also not affect the precision as much."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
